{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88ea6c4",
   "metadata": {},
   "source": [
    "## 1) Imports and plotting style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9966dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance_matrix\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "from matplotlib.pyplot import cm\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7a967",
   "metadata": {},
   "source": [
    "## 2) Analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a593fa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelsize=0.090#in µm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453dcb87",
   "metadata": {},
   "source": [
    "## 3) Info channel order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2384127b",
   "metadata": {},
   "source": [
    "### channel order of current dataset:\n",
    "Ch1: BASP1\n",
    "\n",
    "Ch2: Map2\n",
    "\n",
    "Ch3: Drebrin\n",
    "\n",
    "Ch4: Dapi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f6ce6f",
   "metadata": {},
   "source": [
    "## 4) Input paths and folder structure - modify directory here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cda8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "parentDirectory=r\"add your path here\\\\\" #folder taht has been created by imageJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e636c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=parentDirectory+\"Tables_bgsubtracted\"+\"\\\\\"\n",
    "badFiles=[\"\"]\n",
    "\n",
    "\n",
    "files=[]\n",
    "tmp=os.listdir(path)\n",
    "for f in tmp:\n",
    "    if (\".csv\" in f)==True:  \n",
    "        if (\"segmentCh1+measureCh1\" in f)==True: \n",
    "            if(f[f.find(\"noBGMAX_\")+4:f.find(\".tif\")+4] in badFiles)==False:\n",
    "                files=files+[f]\n",
    "SCh1_ICh1files=files\n",
    "\n",
    "\n",
    "files=[]\n",
    "tmp=os.listdir(path)\n",
    "for f in tmp:\n",
    "    if (\".csv\" in f)==True:  \n",
    "        if (\"segmentCh1+measureCh3\" in f)==True:\n",
    "            if(f[f.find(\"noBGMAX_\")+4:f.find(\".tif\")+4] in badFiles)==False:\n",
    "                files=files+[f]\n",
    "SCh1_ICh3files=files\n",
    "\n",
    "files=[]\n",
    "tmp=os.listdir(path)\n",
    "for f in tmp:\n",
    "    if (\".csv\" in f)==True:  \n",
    "        if (\"segmentCh3+measureCh1\" in f)==True:\n",
    "            if(f[f.find(\"noBGMAX_\")+4:f.find(\".tif\")+4] in badFiles)==False:\n",
    "                files=files+[f]\n",
    "SCh3_ICh1files=files\n",
    "\n",
    "\n",
    "files=[]\n",
    "tmp=os.listdir(path)\n",
    "for f in tmp:\n",
    "    if (\".csv\" in f)==True:  \n",
    "        if (\"segmentCh3+measureCh3\" in f)==True:\n",
    "            if(f[f.find(\"noBGMAX_\")+4:f.find(\".tif\")+4] in badFiles)==False:\n",
    "                files=files+[f]\n",
    "SCh3_ICh3files=files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a86d122",
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "path2=parentDirectory+\"AxonLength\"+\"\\\\\"\n",
    "\n",
    "\n",
    "files=[]\n",
    "tmp=os.listdir(path2)\n",
    "for f in tmp:\n",
    "    if (\".txt\" in f)==True:\n",
    "        if(f[:f.find(\".tif\")+4] in badFiles)==False:\n",
    "            files=files+[f]\n",
    "axoonLengthFiles=files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5b7e8",
   "metadata": {},
   "source": [
    "## 5) Helper: split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c2936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(matrix,identifyer,axis):\n",
    "    splitted=matrix.filter(like=identifyer,axis=axis)\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b4369e",
   "metadata": {},
   "source": [
    "## 6) Load intensity tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c80a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCh1_ICh1_MaxIntensity=[]\n",
    "for f in SCh1_ICh1files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh1_ICh1_MaxIntensity+=[tmp['Max'].values.tolist()]\n",
    "SCh1_ICh1_MaxIntensity=pd.DataFrame(SCh1_ICh1_MaxIntensity,index=SCh1_ICh1files).T.drop(0)\n",
    "    \n",
    "    \n",
    "SCh1_ICh3_MaxIntensity=[]\n",
    "for f in SCh1_ICh3files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh1_ICh3_MaxIntensity+=[tmp['Max'].values.tolist()]\n",
    "SCh1_ICh3_MaxIntensity=pd.DataFrame(SCh1_ICh3_MaxIntensity,index=SCh1_ICh3files).T.drop(0)\n",
    "    \n",
    "    \n",
    "SCh3_ICh1_MaxIntensity=[]\n",
    "for f in SCh3_ICh1files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh3_ICh1_MaxIntensity+=[tmp['Max'].values.tolist()]\n",
    "SCh3_ICh1_MaxIntensity=pd.DataFrame(SCh3_ICh1_MaxIntensity,index=SCh3_ICh1files).T.drop(0)\n",
    "    \n",
    "    \n",
    "SCh3_ICh3_MaxIntensity=[]\n",
    "for f in SCh3_ICh3files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh3_ICh3_MaxIntensity+=[tmp['Max'].values.tolist()]\n",
    "SCh3_ICh3_MaxIntensity=pd.DataFrame(SCh3_ICh3_MaxIntensity,index=SCh3_ICh3files).T.drop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deae1a0c",
   "metadata": {},
   "source": [
    "## 6) Load area tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCh1_ICh1_Area=[]\n",
    "for f in SCh1_ICh1files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh1_ICh1_Area+=[tmp['Area'].values.tolist()]\n",
    "SCh1_ICh1_Area=pd.DataFrame(SCh1_ICh1_Area,index=SCh1_ICh1files).T.drop(0)\n",
    "    \n",
    "    \n",
    "SCh1_ICh3_Area=[]\n",
    "for f in SCh1_ICh3files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh1_ICh3_Area+=[tmp['Area'].values.tolist()]\n",
    "SCh1_ICh3_Area=pd.DataFrame(SCh1_ICh3_Area,index=SCh1_ICh3files).T.drop(0)\n",
    "    \n",
    "    \n",
    "SCh3_ICh1_Area=[]\n",
    "for f in SCh3_ICh1files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh3_ICh1_Area+=[tmp['Area'].values.tolist()]\n",
    "SCh3_ICh1_Area=pd.DataFrame(SCh3_ICh1_Area,index=SCh3_ICh1files).T.drop(0)\n",
    "    \n",
    "    \n",
    "SCh3_ICh3_Area=[]\n",
    "for f in SCh3_ICh3files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh3_ICh3_Area+=[tmp['Area'].values.tolist()]\n",
    "SCh3_ICh3_Area=pd.DataFrame(SCh3_ICh3_Area,index=SCh3_ICh3files).T.drop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cad9cf7",
   "metadata": {},
   "source": [
    "## 7) Axon length summary table and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b2770a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conditions=[\"_\"]\n",
    "axonLength=[]\n",
    "for f in axoonLengthFiles:\n",
    "    axonLength+=[pd.read_table(path2+f,sep=\": \",header=None)[1].values[0]]\n",
    "axonLength=pd.DataFrame(axonLength,index=axoonLengthFiles,columns=[\"axon length\"])\n",
    "axonLength.to_excel(path+\"/axonLength.xlsx\")\n",
    "\n",
    "\n",
    "figure=plt.figure(figsize=(4,4))\n",
    "counter=0\n",
    "for c in conditions:\n",
    "    mean=split(axonLength,c,0).mean()\n",
    "    sem=scipy.stats.sem(split(axonLength,c,0),axis=0)\n",
    "\n",
    "#inSynapses=pd.DataFrame(inSynapses,index=conditions).T*100\n",
    "#inSynapses.index=[\"Percent BASP1 clusters in Synapses\"]\n",
    "  \n",
    "    plt.bar(x=counter,height=mean,yerr=sem,label=c)\n",
    "    plt.xticks([])\n",
    "    plt.title(\"Axon length based on ch2\")\n",
    "    plt.ylabel(\"Axon length [µm]\")\n",
    "    counter+=1\n",
    "plt.legend(loc=(1.05, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/Axon Length.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3bf5e",
   "metadata": {},
   "source": [
    "## 8) Area distributions histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas=[1,1,1,1,]\n",
    "\n",
    "chname1=\"BASP1\"\n",
    "chname3=\"Drebrin\"\n",
    "conditions=[\"_\"]\n",
    "counter=0\n",
    "for c in conditions:    \n",
    "        \n",
    "    \n",
    "    x=split(SCh1_ICh1_Area,c,1).values.ravel()       \n",
    "    x=x[np.logical_not(np.isnan(x))]\n",
    "\n",
    "    plt.hist(x,bins=50,range=(0,6),density=True,alpha=alphas[counter],label=c,histtype=u'step')\n",
    "    counter+=1\n",
    "    \n",
    "plt.xlabel(\"Area [µm²]\")\n",
    "plt.ylabel(\"Counts\")\n",
    "plt.legend()\n",
    "plt.title(\"Area: \"+chname1+ \" Cluster\")\n",
    "plt.show()\n",
    "counter=0 \n",
    "for c in conditions:    \n",
    "     \n",
    "    y=split(SCh3_ICh3_Area,c,1).values.ravel()\n",
    "    y=y[np.logical_not(np.isnan(y))]\n",
    "    plt.hist(y,bins=50,range=(0,6),density=True,alpha=alphas[counter],label=c,histtype=u'step')\n",
    "    counter+=1\n",
    "    \n",
    "plt.xlabel(\"Area [µm²]\")\n",
    "plt.ylabel(\"Counts\")    \n",
    "plt.legend()\n",
    "plt.title(\"Area: \"+chname3+ \" Cluster\")\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c035a8c0",
   "metadata": {},
   "source": [
    "## 9) Otsu-based positivity and synapse area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d74c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "def getOtsus(array):# pd dataframe\n",
    "    threshs=[]\n",
    "    for col in array.T.values:\n",
    "        try:\n",
    "            col =  col[np.isfinite(col)]\n",
    "            threshs+=[filters.threshold_multiotsu(col,classes=3)[1]]\n",
    "        except:\n",
    "            print(\"threshold error, see otsu function  definitin\")\n",
    "            threshs+=[0]\n",
    "    return np.asarray(threshs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a9d93f",
   "metadata": {},
   "source": [
    "## 10) Area bar graph and summary table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d07b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "saveme=[]\n",
    "\n",
    "alphas=[1,1,1,1,]\n",
    "\n",
    "chname1=\"BASP1\"\n",
    "chname3=\"Drebrin\"\n",
    "conditions=[\"_\"]\n",
    "counter=0\n",
    "for c in conditions:    \n",
    "        \n",
    "    positive=SCh3_ICh3_Area*(SCh3_ICh1_MaxIntensity>=getOtsus(SCh3_ICh1_MaxIntensity)).values\n",
    "    negative=SCh3_ICh3_Area*(SCh3_ICh1_MaxIntensity<getOtsus(SCh3_ICh1_MaxIntensity)).values\n",
    "    \n",
    "    x=split(positive,c,1)\n",
    "    x=x[x>0].values\n",
    "    x=np.nanmedian(x,axis=0)\n",
    "    plt.bar(x=counter,height=x.mean(),yerr=scipy.stats.sem(x))\n",
    "    counter+=1\n",
    "    \n",
    "    saveme+=[x.tolist()]\n",
    "\n",
    "\n",
    "    x=split(negative,c,1)\n",
    "    x=x[x>0].values\n",
    "    x=np.nanmedian(x,axis=0)\n",
    "    plt.bar(x=counter,height=x.mean(),yerr=scipy.stats.sem(x))\n",
    "    counter+=1\n",
    "    \n",
    "    saveme+=[x.tolist()]\n",
    "\n",
    "plt.ylabel(\"Area [µm²]\")\n",
    "plt.legend([\"positive\",\"negative\"])\n",
    "plt.title(\"Synapse Area\")\n",
    "plt.xticks(visible=False)\n",
    "plt.savefig(path+\"\\\\_SynapseSize.png\")\n",
    "\n",
    "saveme=pd.DataFrame(saveme,columns=split(positive,c,1).columns,index=[\"positive\",\"negative\"]).T\n",
    "saveme.to_excel(path+\"\\\\_SynapseSize.xlsx\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a1724a",
   "metadata": {},
   "source": [
    "## 11) Utilities: normalize() and binData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c970a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(raw):\n",
    "    #normalized=(raw-raw.min())/(raw.max()-raw.min())#notmalize 1-0\n",
    "    #normalized=(raw/raw.median())#normalize to median\n",
    "    normalized=(raw/raw.sum())#normalize to sum\n",
    "    #normalized=raw\n",
    "    normalized=pd.DataFrame(normalized).fillna(0)\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da51574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binned_statistic\n",
    "\n",
    "def binData(inData,firstcol,lastcol,minbin,maxbin,nbins):\n",
    "    global myArray\n",
    "    myArray=inData.T.values.tolist()\n",
    "    \n",
    "    numcols = len(myArray)\n",
    "    numrows = len(myArray[0]) #[0] for cols. otherwise rows\n",
    "    \n",
    "    if (lastcol==\"max\"):\n",
    "        lastcol=numcols\n",
    "    global j\n",
    "    for j in range(firstcol, lastcol):       \n",
    "        global data\n",
    "        \n",
    "        data = myArray[j]\n",
    "        data = [x for x in data if str(x) != 'nan']\n",
    "        bin_means = binned_statistic(data, data, statistic=\"count\", bins=nbins, range=(minbin, maxbin))[0]\n",
    "        bin_edges= binned_statistic(data, data, statistic='count', bins=nbins, range=(minbin, maxbin))[1]\n",
    "        binnumber= binned_statistic(data, data, statistic='count', bins=nbins, range=(minbin, maxbin))[2]\n",
    "        bin_middle=bin_edges[0:len(bin_edges)-1]+((maxbin-minbin)/nbins/2)\n",
    "\n",
    "        bin_means = np.nan_to_num(bin_means)\n",
    "        bin_means=bin_means.tolist()       \n",
    "        \n",
    "        if (j==0):\n",
    "            allbins = np.vstack((bin_middle, bin_means))\n",
    "        else:\n",
    "            allbins = np.vstack((allbins, bin_means))\n",
    "        \n",
    "    allbins=allbins.T   \n",
    "    \n",
    "    matrix=pd.DataFrame(allbins).set_index(0)\n",
    "    matrix.columns=inData.columns[firstcol:lastcol] \n",
    "    return(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d799911d",
   "metadata": {},
   "source": [
    "## 12) Load XY coordinates for clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6204a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCh1_x=[]\n",
    "SCh1_y=[]\n",
    "for f in SCh1_ICh1files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh1_x+=[tmp['X'].values.tolist()]\n",
    "    SCh1_y+=[tmp['Y'].values.tolist()]\n",
    "SCh1_x=pd.DataFrame(SCh1_x,index=SCh1_ICh1files).T\n",
    "SCh1_y=pd.DataFrame(SCh1_y,index=SCh1_ICh1files).T\n",
    "\n",
    "    \n",
    "SCh3_x=[]\n",
    "SCh3_y=[]\n",
    "for f in SCh3_ICh1files:\n",
    "    tmp=pd.read_csv(path+f)\n",
    "    SCh3_x+=[tmp['X'].values.tolist()]\n",
    "    SCh3_y+=[tmp['Y'].values.tolist()]\n",
    "SCh3_x=pd.DataFrame(SCh3_x,index=SCh3_ICh1files).T\n",
    "SCh3_y=pd.DataFrame(SCh3_y,index=SCh3_ICh1files).T\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d06767",
   "metadata": {},
   "source": [
    "## 13) Nearest-neighbor distances (BASP1 → Drebrin) and BASP1 clusters per synapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c2f4b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chname1=\"BASP1\"\n",
    "chname3=\"Drebrin\"\n",
    "conditions=[\"_\"]\n",
    "\n",
    "print(chname1+\" vs \" +chname3)\n",
    "\n",
    "\n",
    "allNNs=pd.DataFrame()\n",
    "for c in conditions:  \n",
    "    NNs=[] \n",
    "    names=[]\n",
    "    for col in split(SCh1_x,c,axis=1):\n",
    "        xCh1=split(split(SCh1_x,c,axis=1),col,1)\n",
    "        yCh1=split(split(SCh1_y,c,axis=1),col,1)\n",
    "        xCh3=split(split(SCh3_x,c,axis=1),col.replace(\"Ch1+measureCh1\",\"Ch3+measureCh1\"),1)\n",
    "        yCh3=split(split(SCh3_y,c,axis=1),col.replace(\"Ch1+measureCh1\",\"Ch3+measureCh1\"),1) \n",
    "        \n",
    "        \n",
    "        \n",
    "        ####################troubleshooting option##########################\n",
    "\n",
    "        \n",
    "        \n",
    "        #names+=[col[col.find(\"MAX_\")+4:col.find(\".tif\")]]\n",
    "        names+=[col[:]]\n",
    "\n",
    "        subject=pd.DataFrame([xCh1.T.values.tolist()[0],yCh1.T.values.tolist()[0]]).T.dropna().values\n",
    "        reference=pd.DataFrame([xCh3.T.values.tolist()[0],yCh3.T.values.tolist()[0]]).T.dropna().values\n",
    "        if(reference.shape[0]==0):\n",
    "            NN=np.NaN\n",
    "        else:\n",
    "            distanceMatrix=distance_matrix(subject,reference)#calculate distance matrix subject / reference\n",
    "            NN=np.min(distanceMatrix,axis=0)#calculate NN from distance matrix\n",
    "\n",
    "        NNs=NNs+[NN]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    NNs=pd.DataFrame(NNs,index=names).T\n",
    "\n",
    "    plt.title(\"Nearest Neighbor Distances\")\n",
    "    binned=normalize(binData(NNs,0,\"max\",0,1.5,30))\n",
    "    binned.to_excel(path+\"/Distance to nearest_binned \"+chname3+\".xlsx\")\n",
    "    mean=binned.mean(axis=1)\n",
    "    sem=scipy.stats.sem(binned, axis=1)\n",
    "    plt.errorbar(x=binned.index,y=mean,yerr=sem,label=c)\n",
    "    plt.xlabel(\"Distance to nearest \"+chname3+\" [µm]\")\n",
    "    plt.ylabel(\"Normalized Number of \"+chname1+\" clusters\")\n",
    "    plt.legend()\n",
    "    allNNs=pd.concat([allNNs,NNs])\n",
    "    \n",
    "plt.legend(loc=(1.05, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/Distance to nearest \"+chname3+\".png\")\n",
    "allNNs.to_excel(path+\"/Distance to nearest \"+chname3+\".xlsx\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "counter=0\n",
    "figure=plt.figure(figsize=(4,4))\n",
    "for c in conditions:\n",
    "    mean=split(pd.DataFrame(allNNs.median()),c,0).mean()[0]\n",
    "    sem=scipy.stats.sem(split(pd.DataFrame(allNNs.median()),c,0))[0]\n",
    "\n",
    "\n",
    "#inSynapses=pd.DataFrame(inSynapses,index=conditions).T*100\n",
    "#inSynapses.index=[\"Percent BASP1 clusters in Synapses\"]\n",
    "\n",
    "    \n",
    "    plt.bar(x=counter,height=mean,yerr=sem,label=c)\n",
    "    plt.xticks([])\n",
    "    plt.title(\"NN Distance \" + chname3+\" / \"+chname1)\n",
    "    plt.ylabel(\"NN Distance [µm]\")\n",
    "    counter+=1\n",
    "plt.legend(loc=(1.05, 0.5))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(path+\"/NNs.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "fig=plt.figure()\n",
    "synapseSizeuM=1.4\n",
    "counter=0\n",
    "\n",
    "figure=plt.figure(figsize=(4,4))\n",
    "inSynapses=[allNNs[allNNs < synapseSizeuM ].count()/allNNs.count()]\n",
    "inSynapses=pd.DataFrame(inSynapses)\n",
    "\n",
    "for c in conditions:\n",
    "    mean=split(inSynapses,c,1).mean(axis=1)\n",
    "    sem=scipy.stats.sem(split(inSynapses,c,1),axis=1)\n",
    "\n",
    "\n",
    "    plt.bar(x=counter,height=mean,yerr=sem,label=c)\n",
    "    plt.xticks([])\n",
    "    plt.title(chname1+\" clusters in Synapses (synapse size = 1.4µm)\")\n",
    "    plt.ylabel(chname1+\" clusters in Synapses [%]\")\n",
    "    counter+=1\n",
    "plt.legend(loc=(1.05, 0.5))\n",
    "plt.tight_layout()\n",
    "plt.savefig(path+\"/_BASP1 clusters in Synapses.png\")\n",
    "plt.show()\n",
    "inSynapses.T.to_excel(path+\"/_BASP1 clusters in Synapses.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
